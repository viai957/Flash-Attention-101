# FlashAttention From Scratch ğŸš€

## **Overview**
This repository is a deep dive into **FlashAttention**, implemented from first principles using **Triton**, **CUDA Kernels**, and **PyTorch**. The goal is to provide an in-depth understanding of the FlashAttention mechanism by breaking it down step-by-step and implementing it from scratch. 

Key highlights:
- Mathematical derivation with **handwritten notes** ğŸ“œ
- **Jupyter Notebook scratchpad** with explanations at each step
- **Triton and CUDA kernel implementations** âš¡
- **Benchmarking**: Comparing naive PyTorch vs. Triton implementations ğŸ”¥
- **Google Colab Notebook** for easy experimentation ğŸ¯


## **Work Under Progress** ğŸ› ï¸
- [x] **Sliding Window Attention** 
- [ ] **MoBA (Mixture of Block Attention)** to enhance block-wise computations for better performance.

---

## **Folder Structure** ğŸ“‚

```markdown
/FlashAttention-From-Scratch
â”‚â”€â”€ cuda/                         # CUDA kernel implementations for FlashAttention
â”‚â”€â”€ notes/                        # Handwritten notes for derivation and explanation
â”‚â”€â”€ triton/                       # Triton-based implementation of FlashAttention
â”‚â”€â”€ FlashAttention_Benchmark.ipynb  # Jupyter notebook comparing naive PyTorch vs Triton FlashAttention
â”‚â”€â”€ flash-attention-notes.ipynb    # Notebook containing detailed notes and derivations
â”‚â”€â”€ scratch-pad.ipynb              # Jupyter scratchpad for experimenting with implementations
â”‚â”€â”€ triton-scratch-pad.ipynb       # Scratchpad for Triton-specific implementations and testing
```

---

## **Understanding FlashAttention** ğŸ§ 
### **1. What is FlashAttention?**
FlashAttention is an **optimized attention mechanism** that significantly reduces the memory overhead of self-attention by:
- **Computing attention in blocks** (reducing memory footprint)
- **Leveraging hardware-efficient optimizations** (such as fused operations)
- **Reducing redundant computation** while maintaining accuracy

This makes FlashAttention particularly effective for **large transformer models**, as it enables scaling without running into memory bottlenecks.

### **2. Mathematical Derivation** âœï¸
The repository includes **handwritten notes** with a step-by-step breakdown of the derivation. Key components covered:
- Standard **Self-Attention** formulation
- How FlashAttention **optimizes memory usage**
- **Rewriting the attention mechanism** in a more hardware-efficient way

You can find these in the `notes/` folder and the **flash-attention-notes.ipynb** notebook.

---

## **Implementation Details** âš™ï¸
### **1. PyTorch Naive Implementation**
The first step is implementing self-attention **naively** in PyTorch. This serves as a baseline to:
- Understand standard attention computations
- Compare performance later

This is included in the `scratch-pad.ipynb` notebook.

### **2. Triton Implementation** âš¡
**Triton** is used to optimize the attention computation with:
- **Parallelized operations** to reduce bottlenecks
- **Optimized memory access** to improve efficiency
- **Fused computation** to minimize redundant operations

This is covered in `triton/` and `triton-scratch-pad.ipynb`.

### **3. CUDA Kernel Implementation** ğŸ¯
For an even deeper understanding, **custom CUDA kernels** are written to:
- Gain fine-grained control over memory layout
- Optimize performance beyond what Triton provides
- Experiment with GPU-specific optimizations

This is covered in the `cuda/` directory.

---

## **Benchmarking & Performance Analysis** ğŸ“Š
The **FlashAttention_Benchmark.ipynb** notebook compares:
- **Naive PyTorch Implementation**
- **Triton Optimized FlashAttention**

Metrics analyzed:
- **Memory usage** ğŸ“‰
- **Execution time** â±ï¸
- **Scalability** across different input sizes ğŸ“ˆ

### **Google Colab for Easy Testing** ğŸ–¥ï¸
Run the benchmark notebook in Google Colab:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_LINK_HERE)

---

## **Setup & Installation** ğŸ› ï¸
### **1. Clone the Repository**
```sh
git clone https://github.com/viai957/Flash-Attention-101
cd flashattention-from-scratch
```

### **2. Install Dependencies**
```sh
pip install torch triton jupyter
```

### **3. Run Jupyter Notebook**
```sh
jupyter notebook
```
Then open the relevant notebook (**flash-attention-notes.ipynb**, **scratch-pad.ipynb**, etc.) to get started.

---

## **Contributing** ğŸ¤
If you'd like to contribute:
1. Fork the repository
2. Create a new branch
3. Submit a pull request

Feel free to open **issues** for discussions or questions!

---

## **Acknowledgements & References** ğŸ“š
- **FlashAttention Paper**: [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
- **Triton Documentation**: [https://triton-lang.org](https://triton-lang.org)
- **CUDA Programming Guide**: [https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone)

---

## **License** ğŸ“œ
This project is licensed under the MIT License. See `LICENSE` for details.

---

### ğŸš€ **Let's build efficient attention together!** ğŸ”¥
