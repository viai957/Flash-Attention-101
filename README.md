# Flash-Attention-101
 Flash Attention from First Principles: Triton &amp; CUDA implementations with handwritten derivations, notebooks, and Colab benchmarks comparing PyTorch and Triton versions.
