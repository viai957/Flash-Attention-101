{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DriverConfig' object has no attribute 'CudaDriver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCudaDriver\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_active_torch_device\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DriverConfig' object has no attribute 'CudaDriver'"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "print(hasattr(triton.runtime.driver.CudaDriver, 'get_active_torch_device'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CudaDriver' object has no attribute 'get_active_torch_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtl\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_active_torch_device\u001b[49m()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@triton\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_kernel\u001b[39m(x_ptr,  \u001b[38;5;66;03m# *Pointer* to first input vector.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m                y_ptr,  \u001b[38;5;66;03m# *Pointer* to second input vector.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# There are multiple 'programs' processing different data. We identify which program\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# we are here:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     pid \u001b[38;5;241m=\u001b[39m tl\u001b[38;5;241m.\u001b[39mprogram_id(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# We use a 1D launch grid so axis is 0.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vigne\\anaconda3\\envs\\pytorch\\lib\\site-packages\\triton\\runtime\\driver.py:24\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_obj()\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CudaDriver' object has no attribute 'get_active_torch_device'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr,  # *Pointer* to first input vector.\n",
    "               y_ptr,  # *Pointer* to second input vector.\n",
    "               output_ptr,  # *Pointer* to output vector.\n",
    "               n_elements,  # Size of the vector.\n",
    "               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "               # NOTE: `constexpr` so it can be used as a shape value.\n",
    "               ):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], device='cuda:0')\n",
      "If you see tensor([0., 0., 0.], device='cuda:0'), then it works\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "a = torch.rand(3, device=\"cuda\")\n",
    "b = a + a\n",
    "b_compiled = add(a, a)\n",
    "print(b_compiled - b)\n",
    "print(\"If you see tensor([0., 0., 0.], device='cuda:0'), then it works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd(\n",
    "    Q, # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    K, # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    V, # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    softmax_scale, #  1 / (HEAD_DIM**0.5)\n",
    "    M, # BATCH_SIZE, NUM_HEADS, SEQ_LEN\n",
    "    O, # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    stride_Q_batch, stride_Q_head, stride_Q_seq, stride_Q_dim,\n",
    "    stride_K_batch, stride_K_head, stride_K_seq, stride_K_dim,\n",
    "    stride_V_batch, stride_V_head, stride_V_seq, stride_V_dim,\n",
    "    stride_O_batch, stride_O_head, stride_O_seq, stride_O_dim,\n",
    "    BATCH_SIZE, \n",
    "    NUM_HEADS: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV : tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "):\n",
    "    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n",
    "\n",
    "    # This indicates which block in the sequence length to process\n",
    "    block_index_q = tl.program_id(0)\n",
    "\n",
    "    # This indicates which head and batch to process. Each program is associated with a single head of a single batch \n",
    "    index_batch_head = tl.program_id(1)\n",
    "    # This indicates which batch this program is associated with (each batch has NUM_HEADS heads)\n",
    "    index_batch = index_batch_head // NUM_HEADS\n",
    "    # This indicate the position of the head in the batch\n",
    "    index_head = index_batch_head % NUM_HEADS\n",
    "\n",
    "    qkv_offset = (\n",
    "        index_batch.to(tl.int64) * stride_Q_batch\n",
    "        + index_head.to(tl.int64) * stride_Q_head\n",
    "    )\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base = Q + qkv_offset,\n",
    "        shape = (SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_Q_seq, stride_Q_dim),\n",
    "        offsets = (block_index_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape = (BLOCK_SIZE_Q, HEAD_DIM),\n",
    "        order=(1,0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base = V + qkv_offset,\n",
    "        shape = (SEQ_LEN, HEAD_DIM),\n",
    "        strides = (stride_V_seq, stride_V_dim),\n",
    "        offsets=(0,0),\n",
    "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n",
    "        order=(1,0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base = K + qkv_offset,\n",
    "        shape = (HEAD_DIM, SEQ_LEN),\n",
    "        strides = (\n",
    "            stride_K_dim,\n",
    "            stride_K_seq,\n",
    "        ),\n",
    "        offsets = (0,0)\n",
    "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n",
    "        order=(1,0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base = O + qkv_offset,\n",
    "        shape = (SEQ_LEN, )\n",
    "    )\n",
    "\n",
    "\n",
    "class TritonAttention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, casual, softmax_scale):\n",
    "        HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]\n",
    "        HEAD_DIM_V = V.shape[-1]\n",
    "\n",
    "        BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
    "\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        stage = 3 if casual else 1\n",
    "\n",
    "        grid = lambda args: (\n",
    "            triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE_Q\"]), # prog_id : 0\n",
    "            BATCH_SIZE * NUM_HEADS, # prog_id : 1\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # M is the logsumexp for the backward pass, one for each query\n",
    "        M = torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device = Q.device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        __attn_fwd[grid](\n",
    "            Q=Q,\n",
    "            K=K,\n",
    "            V=V,\n",
    "            softmax_scale=softmax_scale,\n",
    "            M=M,\n",
    "            O=O,\n",
    "            stride_Q_batch=Q.stride(0),\n",
    "            stride_Q_head=Q.stride(1),\n",
    "            stride_Q_seq=Q.stride(2),\n",
    "            stride_Q_dim=Q.stride(3),\n",
    "            stride_K_batch=K.stride(0),\n",
    "            stride_K_head=K.stride(1),\n",
    "            stride_K_seq=K.stride(2),\n",
    "            stride_K_dim=K.stride(3),\n",
    "            stride_V_batch=V.stride(0),\n",
    "            stride_V_head=V.stride(1),\n",
    "            stride_V_seq=V.stride(2),\n",
    "            stride_V_dim=V.stride(3),\n",
    "            stride_O_batch=O.stride(0),\n",
    "            stride_O_head=O.stride(1),\n",
    "            stride_O_seq=O.stride(2),\n",
    "            stride_O_dim=O.stride(3),\n",
    "            BATCH_SIZE=Q.shape[0],\n",
    "            NUM_HEADS=Q.shape[1],\n",
    "            SEQ_LEN=Q.shape[2],\n",
    "            HEAD_DIM=HEAD_DIM_K,\n",
    "            STAGE=stage,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.softmax_scale = softmax_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM\n",
    "        ctx.casual = casual\n",
    "        return 0\n",
    "    \n",
    "def test_op(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, casual, dtype=torch.float16):\n",
    "    Q = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_()\n",
    "    )\n",
    "\n",
    "    K = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_()\n",
    "    )\n",
    "\n",
    "    V = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_()\n",
    "    )\n",
    "\n",
    "    softmax_scale = 1 / (HEAD_DIM**0.5)\n",
    "    dO = torch.randn_like(Q)\n",
    "\n",
    "    # naive implementation\n",
    "    MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device=\"cuda\"))\n",
    "    P = torch.mathmul(Q, K.transpose(2, 3)) * softmax_scale\n",
    "    if casual:\n",
    "        P[:, :, MASK == 0] = float(\"-inf\")\n",
    "    P = torch.softmax(P.float(), dim=-1).half()\n",
    "    ref_O = torch.matmul(P,V)\n",
    "    ref_O.backward(dO)\n",
    "    ref_dV, V.grad = V.grad.clone(), None\n",
    "    ref_dK, K.grad = K.grad.clone(), None\n",
    "    ref_dQ, Q.grad = Q.grad.clone(), None\n",
    "\n",
    "    # triton implementation\n",
    "    tri_out = TritonAttention.apply(Q, K, V, casual, softmax_scale).half()\n",
    "    tri_out.backward(dO)\n",
    "    tri_dV, V.grad = V.grad.clone(), None\n",
    "    tri_dK, K.grad = K.grad.clone(), None\n",
    "    tri_dQ, Q.grad = Q.grad.clone(), None\n",
    "\n",
    "    # compare\n",
    "    rtol = 0.0\n",
    "    atol = 1e-2\n",
    "    assert torch.allclose(ref_O, tri_out, atol=atol, rtol=rtol)\n",
    "    assert torch.allclose(ref_dK, tri_dK, atol=atol, rtol=rtol)\n",
    "    assert torch.allclose(ref_dV, tri_dV, atol=atol, rtol=rtol)\n",
    "    assert torch.allclose(ref_dQ, tri_dQ, atol=atol, rtol=rtol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritham\n",
    "## 1. Matrices Q, K, V are split into blocks of size BLOCK_SIZE_Q, BLOCK_SIZE_KV\n",
    "# Divide the output O belongs to the same block as Q\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
