{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton \n",
    "import triton.language as tl\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "NUM_HEADS = 6\n",
    "SEQ_LEN = 4\n",
    "HEAD_DIM = 128\n",
    "\n",
    "Q = (\n",
    "    torch.empty(\n",
    "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=torch.float32, device=\"cuda:0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "K = (\n",
    "    torch.empty(\n",
    "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=torch.float32, device=\"cuda:0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "V = (\n",
    "    torch.empty(\n",
    "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=torch.float32, device=\"cuda:0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "softmax_scale = 1 / (HEAD_DIM ** 0.5)\n",
    "dO = torch.randn_like(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device=\"cuda:0\"))\n",
    "MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale # Q[B, H, S, D] * K[B, H, D, S]  * 1/sqrt(dk)= P[B, H, S, S]\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[:, :, MASK == 0] = float(\"-inf\")\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive implementation \n",
    "P = torch.softmax(P.float(), dim=-1)\n",
    "ref_O = torch.matmul(P, V) # P[B, H, S, S] * V[B, H, S, D] = O[B, H, S, D]\n",
    "ref_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE_Q = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def attn_fwd(Q, K, V, softmax_scale, M, O, strides, BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, BLOCK_SIZE_Q, BLOCK_SIZE_KV, triton_grid, triton_block):\n",
    "    stride_Q_batch, stride_Q_head, stride_Q_seq, stride_Q_dim = strides['Q']\n",
    "    stride_K_batch, stride_K_head, stride_K_seq, stride_K_dim = strides['K']\n",
    "    stride_V_batch, stride_V_head, stride_V_seq, stride_V_dim = strides['V']\n",
    "    stride_O_batch, stride_O_head, stride_O_seq, stride_O_dim = strides['O']\n",
    "\n",
    "    for batch_idx in range(BATCH_SIZE):\n",
    "        for head_idx in range(NUM_HEADS):\n",
    "            for block_idx in range(triton_grid[0]):\n",
    "                seq_start = block_idx * BLOCK_SIZE_Q\n",
    "                seq_end = min(seq_start + BLOCK_SIZE_Q, SEQ_LEN)\n",
    "\n",
    "                print(f\"Batch: {batch_idx}, Head: {head_idx}, Block: {block_idx}, Seq range: {seq_start}:{seq_end}\")\n",
    "\n",
    "                # Perform operations on the block of queries\n",
    "                # Example: accessing a block of P \n",
    "                P_block = P[batch_idx, head_idx, seq_start:seq_end, :]\n",
    "                print(P_block)\n",
    "\n",
    "                # Using strides to access elements\n",
    "                for i in range(seq_start)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triton_like_Attention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, casual, softmax_scale, MASK):\n",
    "        HEAD_DIM_Q, HEAD_DIM_K, HEAD_DIM_V = Q.shape[-1], K.shape[-1], V.shape[-1]\n",
    "        HEAD_DIM =  V.shape[-1]\n",
    "\n",
    "        BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
    "\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        stage = 3 if casual else 1\n",
    "\n",
    "        grid = ( \n",
    "            (SEQ_LEN + BLOCK_SIZE_Q -1) // BLOCK_SIZE_Q, # Axis 0\n",
    "            BATCH_SIZE * NUM_HEADS, # Axis 1\n",
    "            1, # Axis 2 (Z in the CUDA launch grid)\n",
    "        )\n",
    "        \n",
    "        # No of parallel programsL [BATCH_SIZE * NUM_HEADS * NUM_BLOCKS_Q]\n",
    "        # M is the logsumexp for the backward pass\n",
    "        M = torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton \n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd(\n",
    "    Q, # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM,\n",
    "    K, # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM,\n",
    "    V, # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM,\n",
    "    softmax_scale, \n",
    "    M,\n",
    "    O, \n",
    "    stride_Q_batch, stride_Q_head, stride_Q_seq, stride_Q_dim,\n",
    "    stride_K_batch, stride_K_head, stride_K_seq, stride_K_dim,\n",
    "    stride_V_batch, stride_V_head, stride_V_seq, stride_V_dim,\n",
    "    stride_O_batch, stride_O_head, stride_O_seq, stride_O_dim,\n",
    "    BATCH_SIZE,\n",
    "    NUM_HEADS: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "):\n",
    "    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n",
    "\n",
    "    # This indicates which block in the sequence length to process\n",
    "    block_index_q = tl.program_id(0)\n",
    "\n",
    "    # This indicates which head and batch to process. Each program is associated with a single head of a single batch\n",
    "    index_batch_head = tl.program_id(1)\n",
    "    # This indicates which batch this program is associated with (each batch has NUM_HEADS heads)\n",
    "    index_batch = index_batch_head // NUM_HEADS\n",
    "    # This indicates the position of the head in the batch\n",
    "    index_head = index_batch_head % NUM_HEADS\n",
    "\n",
    "    # This allows to get the (SEQ_LEN, HEAD_DIM) block in the Q, K, V by selecting indexing it by batch and head\n",
    "    qkv_offset = (\n",
    "        index_batch.to(tl.int64) * stride_Q_batch\n",
    "        + index_head.to(tl.int64) * stride_Q_head\n",
    "    )\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base= Q + qkv_offset, # Q[index_batch, index_head, block_index_q * BLOCK_SIZE_Q: , :]\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_Q_seq, stride_Q_dim),\n",
    "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "        order=(1,0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base = V + qkv_offset, # V[index_batch, index_head, : , : ]\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_V_seq, stride_V_dim),\n",
    "        offsets=(0,0),\n",
    "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n",
    "        order=(1,0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base = K + qkv_offset,\n",
    "        shape = (SEQ_LEN, HEAD_DIM),\n",
    "        strides = (\n",
    "            stride_K_dim,\n",
    "            stride_K_seq,\n",
    "        ), \n",
    "        offsets=(0,0),\n",
    "        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),\n",
    "        order=(0,1),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base = O + qkv_offset,\n",
    "        shape = (SEQ_LEN, HEAD_DIM),\n",
    "        strides = (stride_O_seq, stride_O_dim),\n",
    "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "        oprder=(1,0),\n",
    "    )\n",
    "\n",
    "    # offs_q: the offsets for the tokens in the Q to process\n",
    "    offs_q = block_index_q * BLOCK_SIZE_Q \n",
    "\n",
    "    # offs_kv: the offsets for the tokens in the K and V sequence to process\n",
    "    offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n",
    "\n",
    "    # m_i: The running maximum. We have one for each query\n",
    "    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")\n",
    "\n",
    "    # l_i: The logsumexp. We have one for each query\n",
    "    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0\n",
    "\n",
    "    # acc: the accumulator for the output, which is a group of rows of the O matrix\n",
    "    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # Load the blocks of Q: it will stay in the SRAM throughout\n",
    "    Q_block = tl.load(Q_block_ptr)\n",
    "\n",
    "    # Stage: 3 if casual, 1 otherwise\n",
    "\n",
    "    if STAGE == 1 or STAGE == 3:\n",
    "        # This step runs for non-causal attention or for the blocks to the left of the diagonal in the causla attention\n",
    "        \n",
    " \n",
    "\n",
    "class TritonAttention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, casual, softmax_scale):\n",
    "        HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]\n",
    "        HEAD_DIM_V = V.shape[-1]\n",
    "\n",
    "        BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
    "\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        stage = 3 if casual else 1\n",
    "\n",
    "        grid = lambda args: (\n",
    "            # ceil(SEQ_LEN / BLOCK_SIZE_Q) = How many blocks of queries do we have ?\n",
    "            triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE_Q\"]), # Which group of queries are we going to work with ?\n",
    "            BATCH_SIZE * NUM_HEADS,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        M = torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        __attn_fwd[grid](\n",
    "            Q=Q,\n",
    "            K=K,\n",
    "            V=V,\n",
    "            softmax_scale=softmax_scale,\n",
    "            M=M,\n",
    "            O=O,\n",
    "            stride_Q_batch=Q.stride(0),\n",
    "            stride_Q_head=Q.stride(1),\n",
    "            stride_Q_seq=Q.stride(2),\n",
    "            stride_Q_dim=Q.stride(3),\n",
    "            stride_K_batch=K.stride(0),\n",
    "            stride_K_head=K.stride(1),\n",
    "            stride_K_seq=K.stride(2),\n",
    "            stride_K_dim=K.stride(3),\n",
    "            stride_V_batch=V.stride(0),\n",
    "            stride_V_head=V.stride(1),\n",
    "            stride_V_seq=V.stride(2),\n",
    "            stride_V_dim=V.stride(3),\n",
    "            stride_O_batch=O.stride(0),\n",
    "            stride_O_head=O.stride(1),\n",
    "            stride_O_seq=O.stride(2),\n",
    "            stride_O_dim=O.stride(3),\n",
    "            BATCH_SIZE=Q.shape[0],\n",
    "            NUM_HEADS=Q.shape[1],\n",
    "            SEQ_LEN=Q.shape[2],\n",
    "            HEAD_DIM=HEAD_DIM_K,\n",
    "            STAGE=stage,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.softmax_scale = softmax_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM\n",
    "        ctx.casual = casual\n",
    "        return 0\n",
    "    \n",
    "def test_op(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, causal, dtype=torch.float16):\n",
    "    Q = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_()\n",
    "    )\n",
    "    K = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_()\n",
    "    )\n",
    "    V = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_()\n",
    "    )\n",
    "\n",
    "    softmax_scale = 1 / (HEAD_DIM**0.5)\n",
    "    dO = torch.randn_like(Q)\n",
    "\n",
    "    # naive implementation\n",
    "    MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device=\"cuda\"))\n",
    "    P = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale\n",
    "    if causal:\n",
    "        P[:, :, MASK == 0] = float(\"-inf\")\n",
    "    P = torch.softmax(P.float(), dim=-1).half()\n",
    "    ref_O = torch.matmul(P, V)\n",
    "    ref_O.backward(dO)\n",
    "    ref_dV, V.grad = V.grad.clone(), None\n",
    "    ref_dK, K.grad = K.grad.clone(), None\n",
    "    ref_dQ, Q.grad = Q.grad.clone(), None\n",
    "\n",
    "    # triton implementation\n",
    "    tri_out = TritonAttention.apply(Q, K, V, causal, softmax_scale).half()\n",
    "    tri_out.backward(dO)\n",
    "    tri_dV, V.grad = V.grad.clone(), None\n",
    "    tri_dK, K.grad = K.grad.clone(), None\n",
    "    tri_dQ, Q.grad = Q.grad.clone(), None\n",
    "\n",
    "    # compare\n",
    "    rtol = 0.0\n",
    "    atol = 1e-2\n",
    "    assert torch.allclose(ref_O, tri_out, atol=atol, rtol=rtol)\n",
    "    assert torch.allclose(ref_dK, tri_dK, atol=atol, rtol=rtol)\n",
    "    assert torch.allclose(ref_dV, tri_dV, atol=atol, rtol=rtol)\n",
    "    assert torch.allclose(ref_dQ, tri_dQ, atol=atol, rtol=rtol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
