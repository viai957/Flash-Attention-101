{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudocode\n",
    "\n",
    "\\section*{Pseudocode}\n",
    "\n",
    "1. Initialize \\( M_0 = -infty \\)\n",
    "2. For \\( i = 1 \\) to \\( N \\):\n",
    "    \\[\n",
    "        M_i = max(M_{i-1}, X_i)\n",
    "    \\]\n",
    "3. Initialize \\( L_0 = 0 \\)\n",
    "4. For \\( J = 1 \\) to \\( N \\):\n",
    "    \\[\n",
    "        L_J = L_{J-1} + e^{X_J - M_N}\n",
    "    \\]\n",
    "5. For \\( k = 1 \\) to \\( N \\):\n",
    "    \\[\n",
    "        X_k \\gets \\frac{e^{X_k - M_N}}{L_N}\n",
    "    \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8., 5., 1., 5., 6., 8., 3., 3., 5., 8.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conventional Softmax\n",
    "import torch\n",
    "\n",
    "tensor = torch.randint(0, 10, (1, 10)).float()\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "# Finding the maximum value\n",
    "m = float(-torch.inf)\n",
    "for x in tensor[0]:\n",
    "    m = max(m, x.item())\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0497870668768883\n",
      "1.0506989488494582\n",
      "1.1004860157263465\n",
      "1.2358212972176261\n",
      "2.235821297217626\n",
      "2.242559244215954\n",
      "2.249297191214282\n",
      "2.2990842580911703\n",
      "3.2990842580911703\n"
     ]
    }
   ],
   "source": [
    "# Computhing the normalization factor\n",
    "l = 0\n",
    "for x in tensor[0]:\n",
    "    l += torch.exp(x - m).item()\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the softmax to each element\n",
    "softmax_row = [(torch.exp(x - m)/l).item() for x in tensor[0]]\n",
    "result = []\n",
    "result.append(softmax_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3031144142150879,\n",
       "  0.015091178007423878,\n",
       "  0.0002764045784715563,\n",
       "  0.015091178007423878,\n",
       "  0.041022077202796936,\n",
       "  0.3031144142150879,\n",
       "  0.0020423689857125282,\n",
       "  0.0020423689857125282,\n",
       "  0.015091178007423878,\n",
       "  0.3031144142150879]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor: tensor([[3., 1., 1., 6., 0., 5., 2., 7., 3., 9.]])\n",
      "Softmax Result: [[0.002048383466899395, 0.0002772185252979398, 0.0002772185252979398, 0.04114287719130516, 0.0001019830015138723, 0.015135619789361954, 0.0007535581244155765, 0.11183793842792511, 0.002048383466899395, 0.8263767957687378]]\n"
     ]
    }
   ],
   "source": [
    "# Consolidated Function \n",
    "from typing import List, Optional, Union, Tuple\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def softmax_row(tensor: torch.Tensor) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Computes the softmax for a single row tensor.\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor of shape (1, N).\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: Softmax values for the row as a nested list.\n",
    "    \"\"\"\n",
    "    m = float('-inf')  # Initialize max value\n",
    "    results = []\n",
    "\n",
    "    # Step 1: Compute the maximum value in the row\n",
    "    for x in tensor[0]:\n",
    "        m = max(m, x.item())\n",
    "\n",
    "    # Step 2: Compute the normalization factor (denominator)\n",
    "    l = 0\n",
    "    for x in tensor[0]:\n",
    "        l += torch.exp(x - m).item()\n",
    "\n",
    "    # Step 3: Compute softmax for each element in the row\n",
    "    softmax_row = [(torch.exp(x - m) / l).item() for x in tensor[0]]\n",
    "    results.append(softmax_row)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "tensor = torch.randint(0, 10, (1, 10)).float()\n",
    "softmax_result = softmax_row(tensor)\n",
    "print(\"Input Tensor:\", tensor)\n",
    "print(\"Softmax Result:\", softmax_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494,\n",
       "  0.47483301162719727,\n",
       "  0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3, 4, 1, 2, 3]])\n",
    "softmax_row(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe Softmax\n",
    "#### Pseudocode\n",
    "\n",
    "1. Initialize \\( m_0 = -\\infty \\), \\( l_0 = 0 \\)\n",
    "2. For \\( i = 1 \\) to \\( N \\):\n",
    "    - Compute \\( m_i = \\max(m_{i-1}, X_i) \\)\n",
    "    - Compute \\( l_i = l_{i-1} \\cdot e^{m_{i-1} - m_i} + e^{X_i - m_i} \\)\n",
    "3. For \\( k = 1 \\) to \\( N \\):\n",
    "    - Compute \\( X_k \\gets \\frac{e^{X_k - m_N}}{l_N} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 9., 7., 1., 2., 5., 8., 2., 5., 6.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rigged Softmax\n",
    "import torch\n",
    "\n",
    "tensor = torch.randint(0, 10, (1, 10)).float()\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0015547872336663783,\n",
       "  0.6272459103409497,\n",
       "  0.08488850184024635,\n",
       "  0.00021041755910617083,\n",
       "  0.0005719742380081532,\n",
       "  0.011488409875583918,\n",
       "  0.2307508807124477,\n",
       "  0.0005719742380081532,\n",
       "  0.011488409875583918,\n",
       "  0.03122873408639953]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the local maximum\n",
    "m_prev = float(-torch.inf) \n",
    "l_prev = 0\n",
    "results = [] \n",
    "for i in tensor[0]: \n",
    "    m_curr = max(m_prev, i)\n",
    "    l_curr = l_prev * torch.exp(m_prev - m_curr).item() + torch.exp(i - m_curr).item()\n",
    "    m_prev = m_curr\n",
    "    l_prev = l_curr\n",
    "\n",
    "softmax_row = [torch.exp(x - m_prev).item() / l_prev for x in tensor[0]]\n",
    "results.append(softmax_row)\n",
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_new(tensor: torch.Tensor) -> List[List[torch.Tensor]]:\n",
    "    m_prev = float(-torch.inf)\n",
    "    l_prev = 0\n",
    "    results = []\n",
    "    for i in tensor[0]:\n",
    "        m_curr = max(m_prev, i)\n",
    "        l_curr = l_prev * torch.exp(m_prev - m_curr).item() + torch.exp(i - m_curr).item()\n",
    "        m_prev = m_curr\n",
    "        l_prev = l_curr\n",
    "\n",
    "    softmax_row = [torch.exp(x - m_prev).item() / l_prev for x in tensor[0]]\n",
    "    results.append(softmax_row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3031144142150879,\n",
       "  0.015091178007423878,\n",
       "  0.0002764045784715563,\n",
       "  0.015091178007423878,\n",
       "  0.041022077202796936,\n",
       "  0.3031144142150879,\n",
       "  0.0020423689857125282,\n",
       "  0.0020423689857125282,\n",
       "  0.015091178007423878,\n",
       "  0.3031144142150879],\n",
       " [0.3031144142150879,\n",
       "  0.015091178007423878,\n",
       "  0.0002764045784715563,\n",
       "  0.015091178007423878,\n",
       "  0.041022077202796936,\n",
       "  0.3031144142150879,\n",
       "  0.0020423689857125282,\n",
       "  0.0020423689857125282,\n",
       "  0.015091178007423878,\n",
       "  0.3031144142150879],\n",
       " [0.3031144142150879,\n",
       "  0.015091178007423878,\n",
       "  0.0002764045784715563,\n",
       "  0.015091178007423878,\n",
       "  0.041022077202796936,\n",
       "  0.3031144142150879,\n",
       "  0.0020423689857125282,\n",
       "  0.0020423689857125282,\n",
       "  0.015091178007423878,\n",
       "  0.3031144142150879],\n",
       " [2.789106723355417e-10,\n",
       "  2.6782898589386787e-33,\n",
       "  6.14341615801095e-06,\n",
       "  2.789106723355417e-10,\n",
       "  4.780273739395541e-25,\n",
       "  2.609940782897381e-23,\n",
       "  0.00012339380919001997,\n",
       "  0.9998704195022583,\n",
       "  3.5321712459204884e-24,\n",
       "  5.379488853812136e-32],\n",
       " [0.09367210417985916,\n",
       "  0.25462716817855835,\n",
       "  0.03446003794670105,\n",
       "  0.09367210417985916,\n",
       "  0.03446003794670105,\n",
       "  0.25462716817855835,\n",
       "  0.09367210417985916,\n",
       "  0.09367210417985916,\n",
       "  0.012677139602601528,\n",
       "  0.03446003794670105],\n",
       " [0.018388420343399048,\n",
       "  0.13587307929992676,\n",
       "  0.018388420343399048,\n",
       "  0.36934128403663635,\n",
       "  0.04998490959405899,\n",
       "  0.36934128403663635,\n",
       "  0.018388420343399048,\n",
       "  0.006764722056686878,\n",
       "  0.006764722056686878,\n",
       "  0.006764722056686878],\n",
       " [0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494,\n",
       "  0.47483301162719727,\n",
       "  0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494],\n",
       " [0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494,\n",
       "  0.47483301162719727,\n",
       "  0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494],\n",
       " [0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494,\n",
       "  0.47483301162719727,\n",
       "  0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494],\n",
       " [0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494,\n",
       "  0.47483301162719727,\n",
       "  0.023640543222427368,\n",
       "  0.06426165997982025,\n",
       "  0.17468130588531494]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_row(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.02364054202726851,\n",
       "  0.06426165690335149,\n",
       "  0.17468130082440936,\n",
       "  0.47483299399271744,\n",
       "  0.02364054202726851,\n",
       "  0.06426165690335149,\n",
       "  0.17468130082440936]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_new(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "BATCH_SIZE = 8 \n",
    "SEQ_LEN = 10 \n",
    "NUM_HEADS = 12\n",
    "HEAD_DIM = 128\n",
    "a1  = torch.tensor([[SEQ_LEN, BATCH_SIZE * NUM_HEADS]])\n",
    "grid = torch.zeros_like(a1)\n",
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12, 10, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, dtype=torch.float16)\n",
    "K = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, dtype=torch.float16)\n",
    "V = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, dtype=torch.float16)\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n"
     ]
    }
   ],
   "source": [
    "grid = lambda args: (\n",
    "    (SEQ_LEN + args[\"BLOCK_SIZE_Q\"]-1) // args[\"BLOCK_SIZE_Q\"],\n",
    "    BATCH_SIZE * NUM_HEADS,\n",
    "    1,\n",
    ")\n",
    "\n",
    "args = {\"BLOCK_SIZE_Q\": 4}\n",
    "grid_shape = grid(args)\n",
    "grid_tensor = torch.zeros(grid_shape)\n",
    "print(grid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_index_q = grid(args)[0]\n",
    "block_index_q\n",
    "\n",
    "index_batch_head = torch.arange(BATCH_SIZE * NUM_HEADS)\n",
    "index_batch_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_offset = torch.arange(0, SEQ_LEN, args[\"BLOCK_SIZE_Q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass of the kernel\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    O_block,\n",
    "    l_i,\n",
    "    m_i,\n",
    "    Q_block,\n",
    "    K_block_ptr,\n",
    "    V_block_ptr,\n",
    "    block_index_q,\n",
    "    softmax_scale,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "    offs_q: tl.constexpr,\n",
    "    offs_kv: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "):\n",
    "    # range of values handled by this stage\n",
    "    if STAGE == 1: # LEft part of diagonal\n",
    "        # From 0 to the left of the diagonal\n",
    "        lo, hi = 0, block_index_q * BLOCK_SIZE_Q\n",
    "    elif STAGE == 2: # Exatly Along the diagonal  \n",
    "        # Used only for the block in which there is transition between non-masked and masked keys\n",
    "        lo, hi = block_index_q * BLOCK_SIZE_Q, (block_index_q + 1) * BLOCK_SIZE_Q\n",
    "        lo = tl.multiple_of(lo, BLOCK_SIZE_Q)\n",
    "    else:\n",
    "        # Only used for non-causal attention\n",
    "        lo, hi = 0, SEQ_LEN\n",
    "\n",
    "    K_block_ptr = tl.advance(K_block_ptr, (0, lo)) \n",
    "    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
    "\n",
    "    # loop over k, v and update accumulator \n",
    "    for start_kv in range(lo, hi, BLOCK_SIZE_KV):\n",
    "        # Just let the compiler know that start_n is a multiple of BLOCK_N, so the compiler can do optimizations\n",
    "        start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)\n",
    "\n",
    "        # -- compute qk ----\n",
    "        K_block = tl.load(K_block_ptr)\n",
    "        QK_block = tl.dot(Q_block, K_block)\n",
    "\n",
    "        if STAGE == 2: \n",
    "            # Mask is applied when idx_q > indx_k,v\n",
    "            mask = offs_q[:, None] >= (start_kv + offs_kv[None, :])\n",
    "            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)\n",
    "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1))\n",
    "            QK_block -= m_ij[:, None]\n",
    "        else:\n",
    "            # Compute the maximum value of qk or keep the old max value\n",
    "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1) * softmax_scale)\n",
    "            QK_block = QK_block * softmax_scale - m_ij[:, None]\n",
    "\n",
    "        # Compute the exponential of each dot product, so now we are computing exp(qk_ij - m_ij)\n",
    "        P_block = tl.math.exp(QK_block)\n",
    "        # Compute the sum by rows of the attention scores\n",
    "        l_ij = tl.sum(P_block, 1)\n",
    "\n",
    "        # This is the correction factor for the previous l_i\n",
    "        alpha = tl.math.exp(m_i - m_ij) # previous estimate - current estimate\n",
    "        # Apply the correction factor to the previous l_i and add the new l_ij\n",
    "        l_i = l_i * alpha + l_ij\n",
    "\n",
    "        V_block = tl.load(V_block_ptr)\n",
    "        P_block = P_block.to(tl.float16)\n",
    "        # This computes the following: O_new = P x V + O_old * alpha\n",
    "        O_block = O_block * alpha[:, None]\n",
    "        O_block = tl.dot(P_block, V_block, O_block) # O_block += P_block @ V_block\n",
    "\n",
    "        m_i = m_ij\n",
    "\n",
    "        # Move to the next block of K and V\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0)) # V[Seq_Len, HEAD_DIM]\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV)) # K[HEAD_DIM, Seq_Len]\n",
    "    return O_block, l_i, m_i\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd(\n",
    "    Q,  # [BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM] # Q[index_batch, index_head, :, :]\n",
    "    K,  # [BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM] # K[index_batch, index_head, :, :]\n",
    "    V,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    softmax_scale,\n",
    "    M,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN\n",
    "    O,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    stride_Q_batch,\n",
    "    stride_Q_head,\n",
    "    stride_Q_seq,\n",
    "    stride_Q_dim,\n",
    "    stride_K_batch,\n",
    "    stride_K_head,\n",
    "    stride_K_seq,\n",
    "    stride_K_dim,\n",
    "    stride_V_batch,\n",
    "    stride_V_head,\n",
    "    stride_V_seq,\n",
    "    stride_V_dim,\n",
    "    stride_O_batch,\n",
    "    stride_O_head,\n",
    "    stride_O_seq,\n",
    "    stride_O_dim,\n",
    "    BATCH_SIZE,\n",
    "    NUM_HEADS: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "):\n",
    "    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n",
    "\n",
    "    # This indicate which block in the sequence length to process\n",
    "    block_index_q = tl.program_id(0)\n",
    "\n",
    "    # This indicates which head and batch to process. Each program is associated with a single head of single batch\n",
    "    index_batch_head = tl.program_id(1)\n",
    "    # This indicates which batch this program is associated with (each batch has NUM_HEADS heads)\n",
    "    index_batch = index_batch_head // NUM_HEADS # Select the right Batch  \n",
    "    # This indicate the poisition of the head in the batch # Select the right Head\n",
    "    index_head = index_batch_head % NUM_HEADS\n",
    "\n",
    "    # This allows to get the (SEQ_LEN, HEAD_DIM) block of Q, K, V by selecting indexing it by batch and head\n",
    "    qkv_offset = (\n",
    "       index_batch.to(tl.int64)* stride_Q_batch # Q[index_batch * stride_Q_batch, :, :, :]\n",
    "       + index_head.to(tl.int64) * stride_Q_head # Q[index_batch * stride_Q_batch + index_head * stride_Q_head, :, :]\n",
    "    )\n",
    "\n",
    "    # We are in Q[index_batch, index_head, block_index_q * BLOCK_SIZE_Q :, : ]\n",
    "    Q_block_ptr = tl.make_block_ptr(# Currently pointing the perticular program to be working with\n",
    "      base= Q + qkv_offset, # Q[index_batch, index_head, :, :]\n",
    "      shape=(SEQ_LEN, HEAD_DIM), \n",
    "      strides=(stride_Q_seq, stride_Q_dim),\n",
    "      offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "      block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "      order=(1, 0),\n",
    "    )\n",
    "  \n",
    "    # We are in V[index_batch, index_head, :, :]\n",
    "    V_block_ptr = tl.make_block_ptr( # V[index_batch, index_head, :, :]\n",
    "      base= V + qkv_offset, \n",
    "      shape=(stride_V_seq, stride_V_dim),\n",
    "      offsets=(0,0),\n",
    "      block_shape = (BLOCK_SIZE_KV, HEAD_DIM),\n",
    "      order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # We are in  K[index_batch, index_head, :, :]\n",
    "    \"\"\"\n",
    "    Actually it won't be selecting `everything that is inside` but only the number of elements indicated\n",
    "    by the `block_shape` parameter of each pointer block. You can consider each pointers block to be\n",
    "    a tensor of pointers with the shape indicated by the param `block_shape`\n",
    "    \"\"\"\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "      base = K + qkv_offset,\n",
    "      shape=(HEAD_DIM, SEQ_LEN),\n",
    "      strides=(\n",
    "        stride_K_dim,\n",
    "        stride_K_seq,\n",
    "      ), # We invert the strides w.r.t Q, so we can transpose the matrix\n",
    "      offsets=(0,0),\n",
    "      block_shape=(HEAD_DIM, BLOCK_SIZE_KV),\n",
    "      order=(0,1),\n",
    "    )\n",
    "\n",
    "    # In this the selection of the pointer should exactly indicate the right pointer for writing\n",
    "    # Q[index_batch, index_head, block_index_q * BLOCK_SIZE_Q :, :]\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "      base= O + qkv_offset,\n",
    "      shape=(SEQ_LEN, HEAD_DIM),\n",
    "      strides=(stride_O_seq, stride_O_dim),\n",
    "      offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "      block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "      order=(1,0),\n",
    "    )\n",
    "\n",
    "    # offs_q: the offsets for the tokens in the Q to process\n",
    "    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q) \n",
    "    # Suppose program=0, block_size=4, Q[0, 1, 2, 3], Suppose program=3, block_size=4, Q[13, 14, 15, 16]\n",
    "    \"\"\"Each block of query is made up of block_size_q no of Queries. Each Q is a token and its\n",
    "    dimention is not all the token but only the part of the head_dim \"\"\"\n",
    "    # offs_kv: the offsets for the tokens in the K and V sequence to process\n",
    "    \"\"\"We don't skip any values like Q here bcs we are going to multiply the whole K and V with the Q\"\"\"\n",
    "    offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n",
    "    # For KV Suppose block_size = 4  -> [0, 1, 2, 3]\n",
    "    # m_i : the running maximum of each row. We have one for each query\n",
    "    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")\n",
    "    # l_i: the running sum. We have one for each query (as we sum the attention scores by rows)\n",
    "    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0 # here +1 is to make the log stable\n",
    "    # acc: the accumilator for the output, which is a group of rows of the O matrix\n",
    "    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # load the blocks of Q: it will stay in SRAM throughout\n",
    "    Q_block = tl.load(Q_block_ptr)\n",
    " \n",
    "    # Stage: 3 if casual else 1\n",
    "    if STAGE == 1 or STAGE == 3:\n",
    "      # This step runs for non-casual attention or for the blocks to the left of the diagonal in the casual attention\n",
    "      O_block, l_i, m_i = _attn_fwd_inner(\n",
    "        O_block,\n",
    "        l_i,\n",
    "        m_i,\n",
    "        Q_block,\n",
    "        K_block_ptr,\n",
    "        V_block_ptr,\n",
    "        block_index_q,\n",
    "        softmax_scale,\n",
    "        BLOCK_SIZE_Q,\n",
    "        BLOCK_SIZE_KV,\n",
    "        4 - STAGE,\n",
    "        offs_q,\n",
    "        offs_kv,\n",
    "        SEQ_LEN,\n",
    "      )\n",
    "\n",
    "    if STAGE == 3:\n",
    "      # This step runs for \n",
    "      O_block, l_i, m_i = _attn_fwd_inner(\n",
    "         O_block,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            Q_block,\n",
    "            K_block_ptr,\n",
    "            V_block_ptr,\n",
    "            block_index_q,\n",
    "            softmax_scale,\n",
    "            BLOCK_SIZE_Q,\n",
    "            BLOCK_SIZE_KV,\n",
    "            2,\n",
    "            offs_q,\n",
    "            offs_kv,\n",
    "            SEQ_LEN,\n",
    "      )\n",
    "      # epilogue\n",
    "      m_i += tl.math.log(\n",
    "         l_i\n",
    "      ) # This is needed to compujte the logsumexp for the backward pass\n",
    "      O_block = O_block / l_i[:, None]\n",
    "      m_ptrs = M + index_batch_head * SEQ_LEN + offs_q\n",
    "      tl.store(m_ptrs, m_i)\n",
    "      tl.store(O_block_ptr, O_block.to(O.type.element_ty))\n",
    "\n",
    "\n",
    "class TritonAttention(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, Q, K, V, casual, softmax_scale):\n",
    "    HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]\n",
    "    HEAD_DIM_V = V.shape[-1]\n",
    "    \n",
    "    BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
    "\n",
    "    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_V\n",
    "\n",
    "    O = torch.empty_like(Q)\n",
    "    stage = 3 if casual else 1\n",
    "\n",
    "    grid = lambda args: (\n",
    "        # ceil(SEQ_LEN / BLOCK_SIZE_Q) = How many blocks of Q we have\n",
    "        triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE_Q\"]), # Which group of Queries are we going to work with ?\n",
    "        BATCH_SIZE * NUM_HEADS, # Which head of which batch element are we going to work with ? --- ^\n",
    "        1, # Z is the CUDA launch grid\n",
    "    )\n",
    "\n",
    "    # Number of parallel programs or kernels : (BATCH_SIZE * NUM_HEADS * NUM_BLOCKS_Q)\n",
    "    \n",
    "    # M is logsumexp for the backward pass, one for each query\n",
    "    M = torch.empty(\n",
    "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    _attn_fwd[grid](\n",
    "        Q=Q, # Query \n",
    "        K=K, # Key\n",
    "        V=V, # Value\n",
    "        softmax_scale=softmax_scale, # 1/sqrt(HEAD_DIM)\n",
    "        M=M, # Memory Block (L in psudo code of the paper)\n",
    "        O=O, # Output\n",
    "        stride_Q_batch=Q.stride(0), # \n",
    "        stride_Q_head=Q.stride(1),\n",
    "        stride_Q_seq=Q.stride(2),\n",
    "        stride_Q_dim=Q.stride(3),\n",
    "        stride_K_batch=K.stride(0),\n",
    "        stride_K_head=K.stride(1),\n",
    "        stride_K_seq=K.stride(2),\n",
    "        stride_K_dim=K.stride(3),\n",
    "        stride_V_batch=V.stride(0),\n",
    "        stride_V_head=V.stride(1),\n",
    "        stride_V_seq=V.stride(2),\n",
    "        stride_V_dim=V.stride(3),\n",
    "        stride_O_batch=O.stride(0),\n",
    "        stride_O_head=O.stride(1),\n",
    "        stride_O_seq=O.stride(2),\n",
    "        stride_O_dim=O.stride(3),\n",
    "        BATCH_SIZE=Q.shape[0],\n",
    "        NUM_HEADS=Q.shape[1],\n",
    "        SEQ_LEN=Q.shape[2],\n",
    "        HEAD_DIM=HEAD_DIM_K,\n",
    "        STAGE=stage,\n",
    "    )\n",
    "\n",
    "    ctx.save_for_backward(Q, K, V, O, M)\n",
    "    ctx.grid = grid\n",
    "    ctx.softmax_scale = softmax_scale\n",
    "    ctx.HEAD_DIM = HEAD_DIM_K\n",
    "    ctx.casual = casual\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_op(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, casual, dtype=torch.float16):\n",
    "  Q = (\n",
    "      torch.empty(\n",
    "          (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "      )\n",
    "      .normal_(mean=0.0, std=0.5)\n",
    "      .requires_grad_()\n",
    "  )\n",
    "  K = (\n",
    "      torch.empty(\n",
    "          (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "      )\n",
    "      .normal_(mean=0.0, std=0.5)\n",
    "      .requires_grad_()\n",
    "  )\n",
    "  V = (\n",
    "      torch.empty(\n",
    "          (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "      )\n",
    "      .normal_(mean=0.0, std=0.5)\n",
    "      .requires_grad_()\n",
    "  )\n",
    "  \n",
    "  softmax_scale = 1 / (HEAD_DIM**0.5) # Q K^T / sqrt(HEAD_DIM)\n",
    "  dO = torch.randn_like(0) # Needed for backward pass \n",
    "\n",
    "  # reference implementation (naive implimentation wrt CUDA and PyTorch)\n",
    "  MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device=\"cuda\"))\n",
    "  P = torch.mathmul(Q, K.transpose(2, 3)) * softmax_scale\n",
    "  if casual:\n",
    "    P[:, :, MASK == 0] = float(\"-inf\")\n",
    "  P = torch.softmax(p.float(\"inf\")).half()\n",
    "  ref_O = torch.matmul(P, V)\n",
    "  ref_O .backward()\n",
    "  ref_dV, V.grad = V.grad.clone(), None\n",
    "  ref_dK, K.grad = K.grad.clone(), None\n",
    "  ref_dQ, Q.grad = Q.grad.clone(), None\n",
    "\n",
    "  # triton implimentation \n",
    "  tri_out = TritonAttention.apply(Q,K,V, casual, softmax_scale).half()\n",
    "  tri_out.backward(dO)\n",
    "  tri_dV, V.grad = V.grad.clone(), None\n",
    "  tri_dK, K.grad = K.grad.clone(), None\n",
    "  tri_dQ, Q.grad = Q.grad.clone(), None\n",
    "\n",
    "  # Compare\n",
    "  rtrol = 0.0\n",
    "  atol = 1e-2 \n",
    "  # Absolute Diffrence\n",
    "  assert torch.allclose(ref_O, tri_out, atol=atol, rtol=rtol)\n",
    "  assert torch.allclose(ref_dK, tri_dK, atol=atol, rtol=rtol)\n",
    "  assert torch.allclose(ref_dV, tri_dV, atol=atol, rtol=rtol)\n",
    "  assert torch.allclose(ref_dQ, tri_dQ, atol=atol, rtol=rtol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
